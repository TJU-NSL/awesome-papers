# awesome-papers

## useful website

* Arxiv-CS-RAG: [https://huggingface.co/spaces/bishmoy/Arxiv-CS-RAG](https://huggingface.co/spaces/bishmoy/Arxiv-CS-RAG)
* Papes.cool:
  * [https://papers.cool/arxiv/cs.LG](https://papers.cool/arxiv/cs.LG): Machine Learning
  * [https://papers.cool/arxiv/cs.DC](https://papers.cool/arxiv/cs.DC): Distributed, Parallel, and Cluster Computing
  * [https://papers.cool/arxiv/cs.OS](https://papers.cool/arxiv/cs.OS): Operating Systems
  * [https://papers.cool/arxiv/cs.CL](https://papers.cool/arxiv/cs.CL): Computation and Language



## awesome papers
<!-- -------------------------------------------------------------------------- Template (DE NOT DELETE) -----------------------------------------------------------------------------
[Template] * (conf/trans/arxiv) Paper title [link](http_source_link) [NOTE: key words / author / affiliation]
[Examples] * (NIPS'17) Attention Is All You Need [link](https://arxiv.org/abs/1706.03762) [Attention | Google]
[Examples] * (Arxiv'24) Optimal Block-Level Draft Verification for Accelerating Speculative Decoding [link](https://arxiv.org/abs/2403.10444) [Speculative Decoding | Google]
------------------------------------------------------------------------------- Template (DE NOT DELETE) ----------------------------------------------------------------------------- -->

* to be updated






## [OSDI 2024](https://www.usenix.org/conference/osdi24)
<!-- -------------------------------------------------------------------------- Template (DE NOT DELETE) -----------------------------------------------------------------------------
[Template] * (conf/trans/arxiv) Paper title [link](http_source_link) [NOTE: key words / author / affiliation]
[Examples] * (OSDI'24) Attention Is All You Need [link](https://arxiv.org/abs/1706.03762) [Attention | Google]
[Examples] * (OSDI'24) Optimal Block-Level Draft Verification for Accelerating Speculative Decoding [link](https://arxiv.org/abs/2403.10444) [Speculative Decoding | Google]
------------------------------------------------------------------------------- Template (DE NOT DELETE) ----------------------------------------------------------------------------- -->

* to be updated
* (OSDI'24) Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve [link](https://arxiv.org/abs/2403.02310) []
* (OSDI'24) DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving [link](https://arxiv.org/pdf/2401.09670.pdf) [PKU]





## [NSDI 2024](https://)
<!-- -------------------------------------------------------------------------- Template (DE NOT DELETE) -----------------------------------------------------------------------------
[Template] * (conf/trans/arxiv) Paper title [link](http_source_link) [NOTE: key words / author / affiliation]
[Examples] * (NSDI'24) Attention Is All You Need [link](https://arxiv.org/abs/1706.03762) [Attention | Google]
[Examples] * (NSDI'24) Optimal Block-Level Draft Verification for Accelerating Speculative Decoding [link](https://arxiv.org/abs/2403.10444) [Speculative Decoding | Google]
------------------------------------------------------------------------------- Template (DE NOT DELETE) ----------------------------------------------------------------------------- -->

* to be updated
* (NSDI'24) MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs [link](https://arxiv.org/abs/2402.15627) [Training|Bytedance]




## [MLSys 2024](https://mlsys.org/virtual/2024/papers.html?filter=titles)
<!-- -------------------------------------------------------------------------- Template (DE NOT DELETE) -----------------------------------------------------------------------------
[Template] * (conf/trans/arxiv) Paper title [link](http_source_link) [NOTE: key words / author / affiliation]
[Examples] * (MLSys'24) Attention Is All You Need [link](https://arxiv.org/abs/1706.03762) [Attention | Google]
[Examples] * (MLSys'24) Optimal Block-Level Draft Verification for Accelerating Speculative Decoding [link](https://arxiv.org/abs/2403.10444) [Speculative Decoding | Google]
------------------------------------------------------------------------------- Template (DE NOT DELETE) ----------------------------------------------------------------------------- -->
* [Accepted Papers](https://mlsys.org/Conferences/2024/AcceptedPapers)

#### LLM - serving
* (MLSys'24) HeteGen: Efficient Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices [paper](https://arxiv.org/abs/2403.01164) [**Inference** | **Parallelism** | NUS]
* (MLSys'24) FlashDecoding++: Faster Large Language Model Inference with Asynchronization, Flat GEMM Optimization, and Heuristics [paper](https://arxiv.org/abs/2311.01282) [**Inference** | Tsinghua | SJTU]
* (MLSys'24) VIDUR: A LARGE-SCALE SIMULATION FRAMEWORK FOR LLM INFERENCE [paper]() [**Inference** | **Simulation Framework** | Microsoft]
* (MLSys'24) UniDM: A Unified Framework for Data Manipulation with Large Language Models [paper](https://arxiv.org/abs/2402.03009) [**Inference** | **Memory** | **Long Context** | Alibaba]
* (MLSys'24) SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts Models [paper](https://arxiv.org/abs/2310.18859) [**Serving** | **MoE**]
* (MLSys'24) Keyformer: KV Cache reduction through key tokens selection for Efficient Generative Inference [paper](https://arxiv.org/abs/2403.09054) [**Inference** | **KV Cache**]
* (MLSys'24) Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache [paper](https://arxiv.org/abs/2306.14048) [**Inference** | **KV Cache**]
* (MLSys'24) Prompt Cache: Modular Attention Reuse for Low-Latency Inference [paper](https://arxiv.org/abs/2311.04934) [**Inference** | **KV Cache** | Yale]
* (MLSys'24) SLoRA: Scalable Serving of Thousands of LoRA Adapters [paper](https://arxiv.org/abs/2311.03285) [code](https://github.com/S-LoRA/S-LoRA) [**Serving** | **LoRA** | Stanford | Berkerley]
* (MLSys'24) Punica: Multi-Tenant LoRA Serving [paper](https://arxiv.org/abs/2310.18547) [code](https://github.com/punica-ai/punica) [**Serving** | **LoRA** | Tianqi Chen]

#### LLM - training and quantization
* (MLSys'24) AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration [paper](https://arxiv.org/abs/2306.00978) [code](https://github.com/mit-han-lab/llm-awq) [**Quantization** | MIT]
* (MLSys'24) Efficient Post-training Quantization with FP8 Formats [paper](https://arxiv.org/abs/2309.14592) [**Quantization** | Intel]
* (MLSys'24) Does Compressing Activations Help Model Parallel Training? [paper](https://arxiv.org/abs/2301.02654) [**Quantization**]
* (MLSys'24) Atom: Low-Bit Quantization for Efficient and Accurate LLM Serving [paper](https://arxiv.org/abs/2310.19102) [code](https://github.com/efeslab/Atom) [**Quantization** | **Serving** | SJTU | CMU]
* (MLSys'24) QMoE: Sub-1-Bit Compression of Trillion Parameter Models [paper](https://arxiv.org/abs/2310.16795) [code](https://github.com/IST-DASLab/qmoe) [**Quantization** | **MoE** | Google]
* (MLSys'24) Lancet: Accelerating Mixture-of-Experts Training by Overlapping Weight Gradient Computation and All-to-All Communication [paper]() [**Training** | **MoE** | HKU]
* (MLSys'24) DiffusionPipe: Training Large Diffusion Models with Efficient Pipelines [paper]() [**Training** | **Diffusion** | HKU]

#### ML Serving
* (MLSys'24) FLASH: Fast Model Adaptation in ML-Centric Cloud Platforms [paper](https://haoran-qiu.com/publication/mlsys-2024/) [code](https://gitlab.engr.illinois.edu/DEPEND/flash) [MLsys | UIUC]
* (MLSys'24) ACROBAT: Optimizing Auto-batching of Dynamic Deep Learning at Compile Time [paper](https://arxiv.org/abs/2305.10611) [**Compiling** | **Batching** | CMU]
* (MLSys'24) On Latency Predictors for Neural Architecture Search [paper](https://arxiv.org/abs/2403.02446) [Google]
* (MLSys'24) vMCU: Coordinated Memory Management and Kernel Optimization for DNN Inference on MCUs [paper](https://arxiv.org/abs/2001.03288) [**DNN Inference** | PKU]















